name: auto crawling contest data

on:
  schedule:
    - cron:  '0 13 * * 0' # for weekly contest data (Sunday  13:00)
    - cron:  '0 0 * * 2'  # for weekly contest data (Tuesday 00:00)
    - cron:  '0 1 * * 0'  # for bi-weekly contest (Sunday 01:00)
    - cron:  '0 9 * * 0'  # for bi-weekly contest (Sunday 09:00)

jobs:
  crawling:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
    # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
    - uses: actions/checkout@v2
      with:
        ref: 'gh-pages'
    - uses: actions/setup-python@v1
      with:
        python-version: 3.x
    - name: Install dependencies
      run: |
        python3 -m pip install requests

    - name: crawling for contest data
      run: |
        timestamp=$(date +%s)
        latestContest=$(curl 'https://leetcode.com/graphql' -H 'content-type: application/json' --data-binary '{"operationName":null,"variables":{},"query":"{\n  allContests {\n    title\n    titleSlug\n    startTime\n    duration\n    originStartTime\n}\n}\n"}' --compressed | jq "[.data.allContests[] | select(.startTime < ${timestamp})][0].titleSlug" | sed -e s/\"//g)
        echo "Start crawling contest data for $latestContest"
        python3 ./crawler.py $latestContest
        git config --global user.email "chiehmin18@gmail.com"
        git config --global user.name "[bot] github actions"
        git add .
        git commit -a -m"AUTO UPDATE: update contest data for $latestContest"
        git push origin gh-pages
